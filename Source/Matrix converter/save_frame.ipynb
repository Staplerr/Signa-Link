{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mteer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.components.containers import landmark as mpLandmark\n",
    "from mediapipe.tasks.python import vision\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentDirectory = Path(os.path.abspath(\"\"))\n",
    "out_path = r'C:\\Users\\mteer\\OneDrive\\Desktop\\ikky\\output'\n",
    "inputDirectory = parentDirectory.joinpath(\"Videos\")\n",
    "outputFile = parentDirectory.joinpath(\"image output\" + \".xlsx\")\n",
    "modelDirectory = parentDirectory.joinpath(\"Model\")\n",
    "handModel = modelDirectory.joinpath(\"hand_landmarker.task\")\n",
    "poseModel = modelDirectory.joinpath(\"pose_landmarker_full.task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "supportsExtension = [\"*/*.mp4\", \"*/*.mov\"]\n",
    "sample = 5 #Save frame every n frame\n",
    "frameBuffer = 10\n",
    "processes = []\n",
    "processesCount = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poseColumnNameList = [\"nose\", \"left eye (inner)\", \"left eye\", \"left eye (outer)\", \"right eye (inner)\",\n",
    "                      \"right eye\", \"right eye (outer)\", \"left ear\", \"right ear\", \"mouth (left)\",\n",
    "                      \"mouth (right)\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "                      \"left wrist\", \"right wrist\", \"left pinky\", \"right pinky\", \"left index\",\n",
    "                      \"right index\",\"left thumb\",\"right thumb\",\"left hip\",\"right hip\"]\n",
    "handColumnNameList = [\"wrist\", \"thumb cmc\", \"thumb mcp\", \"thumb ip\", \"thumb tip\",\n",
    "                      \"index finger mcp\", \"index finger pip\", \"index finger dip\", \"index finger tip\", \"middle finger mcp\",\n",
    "                      \"middle finger pip\", \"middle finger dip\", \"middle finger tip\", \"ring finger mcp\", \"ring finger pip\",\n",
    "                      \"ring finger dip\", \"ring finger tip\", \"pinky mcp\", \"pinky pip\", \"pinky dip\",\n",
    "                      \"pinky tip\"]\n",
    "columnNames = [\"Label\"]\n",
    "\n",
    "for i in range(frameBuffer):\n",
    "    for columnName in poseColumnNameList:\n",
    "        columnNames.append(f\"{columnName}_{i}\")\n",
    "    for columnName in handColumnNameList:\n",
    "        columnNames.append(f\"right_{columnName}_{i}\")\n",
    "    for columnName in handColumnNameList:\n",
    "        columnNames.append(f\"left_{columnName}_{i}\")\n",
    "df = pd.DataFrame(columns=columnNames)\n",
    "\n",
    "labelList = {\"กรอบ\": 0,     \"กระเพรา\": 1,    \"ขา\": 2,       \"ข้าว\": 3,\n",
    "             \"ไข่\": 4,       \"คะน้า\": 5,      \"เค็ม\": 6,       \"โจ๊ก\": 7,\n",
    "             \"แดง\": 8,      \"ต้ม\": 9,        \"แตงโม\": 10,    \"น้ำพริกเผา\": 11,\n",
    "             \"บะหมี่\": 12,    \"เปรี้ยว\": 13,    \"ผัด\": 14,       \"ฝรั่ง\": 15,\n",
    "             \"พริกแกง\": 16,  \"มะม่วง\": 17,    \"ม้า\": 18,       \"มาม่า\": 19,\n",
    "             \"ลูกชิ้นปลา\": 20, \"เลือด\": 21,     \"สับ\": 22,       \"เส้นเล็ก\": 23,\n",
    "             \"เส้นใหญ่\": 24,  \"หมู\": 25,       \"หวาน\": 26,     \"องุ่น\": 27,\n",
    "             \"แอปเปิ้ล\": 28}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mediapipe config\n",
    "minPoseConfidence = 0.5\n",
    "minHandConfidence = 0.5\n",
    "\n",
    "baseOptions = mp.tasks.BaseOptions\n",
    "poseLandmarker = vision.PoseLandmarker\n",
    "poseLandmarkerOptions = vision.PoseLandmarkerOptions\n",
    "handLandmarker = vision.HandLandmarker\n",
    "handLandmarkerOptions = vision.HandLandmarkerOptions\n",
    "visionRunningMode = vision.RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711379905.237361    1310 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "I0000 00:00:1711379905.325346    1310 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create the landmarker object\n",
    "poseOption = poseLandmarkerOptions(base_options=baseOptions(model_asset_path=poseModel),\n",
    "                                   running_mode=vision.RunningMode.IMAGE,\n",
    "                                   min_pose_detection_confidence=minPoseConfidence)\n",
    "handOption = handLandmarkerOptions(base_options=baseOptions(model_asset_path=handModel),\n",
    "                                   running_mode=visionRunningMode.IMAGE,\n",
    "                                   min_hand_detection_confidence=minHandConfidence,\n",
    "                                   num_hands=2)\n",
    "poseLandmarker = poseLandmarker.create_from_options(poseOption)\n",
    "handLandmarker = handLandmarker.create_from_options(handOption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilePATHS(directory):\n",
    "    videoPATHs = []\n",
    "    for extension in supportsExtension: #Collect file that has mp4 and mov file extension\n",
    "        for file in directory.glob(extension):\n",
    "            videoPATHs.append(file)\n",
    "    print(f\"total files: {len(videoPATHs)}\")\n",
    "    return videoPATHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitList(list):\n",
    "    step = len(list) // processesCount\n",
    "    remain = len(list) % processesCount\n",
    "    for i in range(0, len(list), step):\n",
    "        yield list[i:i + step + remain] #return multiple 1D list\n",
    "        remain = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "[[ 0.756    1.15    -0.0951 ]\n",
      " [ 0.5254   0.503   -1.14   ]\n",
      " [ 0.5737   0.468   -1.061  ]\n",
      " [ 0.5967   0.4683  -1.061  ]\n",
      " [ 0.6196   0.4688  -1.062  ]\n",
      " [ 0.4858   0.4692  -1.039  ]\n",
      " [ 0.457    0.4707  -1.039  ]\n",
      " [ 0.434    0.473   -1.039  ]\n",
      " [ 0.6562   0.4888  -0.5728 ]\n",
      " [ 0.416    0.4946  -0.4294 ]\n",
      " [ 0.5728   0.5444  -0.9653 ]\n",
      " [ 0.4775   0.5483  -0.9272 ]\n",
      " [ 0.8477   0.7065  -0.396  ]\n",
      " [ 0.2568   0.702   -0.2817 ]\n",
      " [ 0.9556   0.944   -0.4546 ]\n",
      " [-0.02509  0.9375  -1.118  ]\n",
      " [ 0.988    1.193   -0.7783 ]\n",
      " [ 0.2366   0.7554  -2.38   ]\n",
      " [ 1.009    1.272   -0.894  ]\n",
      " [ 0.3198   0.701   -2.627  ]\n",
      " [ 0.9355   1.262   -0.921  ]\n",
      " [ 0.3481   0.6787  -2.506  ]\n",
      " [ 0.912    1.235   -0.806  ]\n",
      " [ 0.3403   0.6934  -2.367  ]\n",
      " [ 0.756    1.15    -0.0951 ]\n",
      " [ 0.329    1.155    0.1009 ]]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "#Re-written of addLandMark and toDataFrame\n",
    "def addLandmarks(coordinates, array):\n",
    "    if type(coordinates[0]) == mpLandmark.NormalizedLandmark:\n",
    "        for landmark in coordinates:\n",
    "            value = np.array([landmark.x, landmark.y, landmark.z], dtype=np.float16)\n",
    "            array = np.vstack([array, value])\n",
    "    else:\n",
    "        for filler in coordinates:\n",
    "            array = np.vstack([array, filler])\n",
    "    return array\n",
    "\n",
    "def generateFrameLandmarks(frame):\n",
    "    frame = mp.Image.create_from_file(frame)\n",
    "    poseResult = poseLandmarker.detect(frame)\n",
    "    handResult = handLandmarker.detect(frame)\n",
    "    poseCoordinates = poseResult.pose_landmarks\n",
    "    handedness = handResult.handedness \n",
    "    handCoordinates = handResult.hand_landmarks\n",
    "\n",
    "    coordinatesArray = np.empty((3, ), dtype=np.float16)\n",
    "    coordinatesArray = addLandmarks(poseCoordinates[0][:25], coordinatesArray)\n",
    "    coordinatesArray = np.delete(coordinatesArray, 0, axis=0) #remove the first element that got create when declaire the empty array\n",
    "    for index, category in enumerate(handedness):\n",
    "        if len(handCoordinates) < 2: #check if mp detect only one hand\n",
    "            filler = np.zeros(shape=(len(handColumnNameList), 3), dtype=np.float16)\n",
    "            if category[index].index == 0: #detect right\n",
    "                coordinatesArray = addLandmarks(handCoordinates[index], coordinatesArray)\n",
    "                coordinatesArray = addLandmarks(filler, coordinatesArray)\n",
    "            else: #detect left\n",
    "                coordinatesArray = addLandmarks(filler, coordinatesArray)\n",
    "                coordinatesArray = addLandmarks(handCoordinates[index], coordinatesArray)\n",
    "            break\n",
    "        else:\n",
    "            coordinatesArray = addLandmarks(handCoordinates[index], coordinatesArray)\n",
    "    print(coordinatesArray)\n",
    "    print(len(coordinatesArray))\n",
    "    return coordinatesArray #return np array\n",
    "\n",
    "bothIMG = parentDirectory.joinpath('Images/กรอบ/IMG_0189_50.png')\n",
    "leftIMG = parentDirectory.joinpath('Images/กระเพรา/IMG_0199_25.png')\n",
    "rightIMG =parentDirectory.joinpath('Images/เปรี้ยว/VID_20240123200118_0.png')\n",
    "\n",
    "path = rightIMG\n",
    "label = path.parent\n",
    "generateFrameLandmarks(str(path), str(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFrames(videoPATHs, nextrow):\n",
    "    for videoPATH in videoPATHs:\n",
    "        flipping = True #don't even know what to name this\n",
    "        frames = []\n",
    "        label = videoPATH.parent.name\n",
    "        print(f\"Adding: {videoPATH.name} to dataframe as: {label} to index: {str(len(df))}\")\n",
    "\n",
    "        cap = cv2.VideoCapture(str(videoPATH))\n",
    "        currentFrame = 0\n",
    "\n",
    "        while cap.isOpened:\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                if currentFrame / sample - currentFrame // sample == 0: #check if the current frame is the sample frame\n",
    "                    frames.append(frame)\n",
    "                    #cv2.imshow(f\"name: {videoPATH} frame: {currentFrame}\", frame)\n",
    "                currentFrame += 1\n",
    "            else:\n",
    "                break\n",
    "        while len(frames) > frameBuffer: #remove frame untile the frames list contain only 10 frame\n",
    "            if flipping:\n",
    "                frames.pop(0)\n",
    "            else:\n",
    "                frames.pop()\n",
    "            flipping = not flipping\n",
    "        cap.release()\n",
    "        toDataFrame(frames, label, nextrow)\n",
    "        nextrow += 1\n",
    "\n",
    "startTime = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/page/code-save/Signa-Link/Source/Matrix converter/Videos\n",
      "total files: 513\n",
      "Adding: VID_20240122202046.mp4 to dataframe as: โจ๊ก to index: 0\n",
      "Adding: VID_20240122202231.mp4 to dataframe as: โจ๊ก to index: 1\n",
      "Adding: VID_20240122201845.mp4 to dataframe as: โจ๊ก to index: 2\n",
      "Adding: VID_20240122201749.mp4 to dataframe as: โจ๊ก to index: 3\n",
      "Adding: VID_20240122202207.mp4 to dataframe as: โจ๊ก to index: 4\n",
      "Adding: VID_20240122202146.mp4 to dataframe as: โจ๊ก to index: 5\n",
      "Adding: VID_20240122202117.mp4 to dataframe as: โจ๊ก to index: 6\n",
      "Adding: VID_20240122202022.mp4 to dataframe as: โจ๊ก to index: 7\n",
      "Adding: VID_20240122201948.mp4 to dataframe as: โจ๊ก to index: 8\n",
      "Adding: VID_20240122201812.mp4 to dataframe as: โจ๊ก to index: 9\n",
      "Adding: VID_20240122193559.mp4 to dataframe as: กรอบ to index: 10\n",
      "Adding: VID_20240122193231.mp4 to dataframe as: กรอบ to index: 11\n",
      "Adding: VID_20240122193532.mp4 to dataframe as: กรอบ to index: 12\n",
      "Adding: VID_20240122193155.mp4 to dataframe as: กรอบ to index: 13\n",
      "Adding: VID_20240122193712.mp4 to dataframe as: กรอบ to index: 14\n",
      "Adding: VID_20240122193631.mp4 to dataframe as: กรอบ to index: 15\n",
      "Adding: VID_20240122193654.mp4 to dataframe as: กรอบ to index: 16\n",
      "Adding: VID_20240122193513.mp4 to dataframe as: กรอบ to index: 17\n",
      "Adding: VID_20240122193349.mp4 to dataframe as: กรอบ to index: 18\n",
      "Adding: VID_20240122203553.mp4 to dataframe as: แตงโม to index: 19\n",
      "Adding: VID_20240122203806.mp4 to dataframe as: แตงโม to index: 20\n",
      "Adding: VID_20240122203734.mp4 to dataframe as: แตงโม to index: 21\n",
      "Adding: VID_20240122203718.mp4 to dataframe as: แตงโม to index: 22\n",
      "Adding: VID_20240122203654.mp4 to dataframe as: แตงโม to index: 23\n",
      "Adding: VID_20240122203611.mp4 to dataframe as: แตงโม to index: 24\n",
      "Adding: VID_20240122203751.mp4 to dataframe as: แตงโม to index: 25\n",
      "Adding: VID_20240122203639.mp4 to dataframe as: แตงโม to index: 26\n",
      "Adding: VID_20240122203622.mp4 to dataframe as: แตงโม to index: 27\n",
      "Adding: VID_20240122203816.mp4 to dataframe as: แตงโม to index: 28\n",
      "Adding: VID_20240122200836.mp4 to dataframe as: คะน้า to index: 29\n",
      "Adding: VID_20240122200700.mp4 to dataframe as: คะน้า to index: 30\n",
      "Adding: VID_20240122200548.mp4 to dataframe as: คะน้า to index: 31\n",
      "Adding: VID_20240122200640.mp4 to dataframe as: คะน้า to index: 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m videoPATHs \u001b[38;5;241m=\u001b[39m getFilePATHS(inputDirectory)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#videoPATHsList = list(splitList(videoPATHs))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#del videoPATHs #hopefully it will freeup some memory\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43msaveFrames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideoPATHs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#nextRow = 0 #don't even know what to name this\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#for i in range(processesCount): #initiate process\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#    p = multiprocessing.Process(target=saveFrames, args=[videoPATHsList[i], nextRow])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#del videoPATHsList\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#del processes\u001b[39;00m\n\u001b[1;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#Shuffle dataframe\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 27\u001b[0m, in \u001b[0;36msaveFrames\u001b[0;34m(videoPATHs, nextrow)\u001b[0m\n\u001b[1;32m     25\u001b[0m     flipping \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m flipping\n\u001b[1;32m     26\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtoDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnextrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m nextrow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m, in \u001b[0;36mtoDataFrame\u001b[0;34m(frames, label, nextRow)\u001b[0m\n\u001b[1;32m     17\u001b[0m frame \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mImage(image_format\u001b[38;5;241m=\u001b[39mmp\u001b[38;5;241m.\u001b[39mImageFormat\u001b[38;5;241m.\u001b[39mSRGB, data\u001b[38;5;241m=\u001b[39m(frame))\n\u001b[1;32m     18\u001b[0m poseResult \u001b[38;5;241m=\u001b[39m poseLandmarker\u001b[38;5;241m.\u001b[39mdetect(frame)\n\u001b[0;32m---> 19\u001b[0m handResult \u001b[38;5;241m=\u001b[39m \u001b[43mHandLandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m poseCoordinates \u001b[38;5;241m=\u001b[39m poseResult\u001b[38;5;241m.\u001b[39mpose_landmarks\n\u001b[1;32m     21\u001b[0m handCoordinates \u001b[38;5;241m=\u001b[39m handResult\u001b[38;5;241m.\u001b[39mhand_landmarks\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/mediapipe/tasks/python/vision/hand_landmarker.py:401\u001b[0m, in \u001b[0;36mHandLandmarker.detect\u001b[0;34m(self, image, image_processing_options)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs hand landmarks detection on the given image.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mOnly use this method when the HandLandmarker is created with the image\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m  RuntimeError: If hand landmarker detection failed to run.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    398\u001b[0m normalized_rect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_to_normalized_rect(\n\u001b[1;32m    399\u001b[0m     image_processing_options, image, roi_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    400\u001b[0m )\n\u001b[0;32m--> 401\u001b[0m output_packets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_image_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_IMAGE_IN_STREAM_NAME\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_creator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_NORM_RECT_STREAM_NAME\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_creator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalized_rect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pb2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_packets[_HAND_LANDMARKS_STREAM_NAME]\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    409\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HandLandmarkerResult([], [], [])\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:95\u001b[0m, in \u001b[0;36mBaseVisionTaskApi._process_image_data\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mode \u001b[38;5;241m!=\u001b[39m _RunningMode\u001b[38;5;241m.\u001b[39mIMAGE:\n\u001b[1;32m     91\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask is not initialized with the image mode. Current running mode:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     93\u001b[0m       \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mode\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     94\u001b[0m   )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(inputDirectory)\n",
    "videoPATHs = getFilePATHS(inputDirectory)\n",
    "#videoPATHsList = list(splitList(videoPATHs))\n",
    "#del videoPATHs #hopefully it will freeup some memory\n",
    "\n",
    "saveFrames(videoPATHs, 0)\n",
    "\n",
    "#nextRow = 0 #don't even know what to name this\n",
    "#for i in range(processesCount): #initiate process\n",
    "#    p = multiprocessing.Process(target=saveFrames, args=[videoPATHsList[i], nextRow])\n",
    "#    p.start()\n",
    "#    processes.append(p)\n",
    "#    nextRow += len(videoPATHsList[i])\n",
    "#for process in processes: #wait for process to end\n",
    "#    process.join()\n",
    "#\n",
    "#del videoPATHsList\n",
    "#del processes\n",
    "\n",
    "df = df.sample(frac=1) #Shuffle dataframe\n",
    "print(\"Output dataframe:\")\n",
    "print(df)\n",
    "df.to_excel(outputFile, index=False)\n",
    "\n",
    "finishTime = time.perf_counter()\n",
    "print(f\"total time: {finishTime - startTime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r'C:\\Users\\mteer\\OneDrive\\Desktop\\ikky'\n",
    "\n",
    "os.chdir(out_path)\n",
    "for file in os.listdir(test_path):\n",
    "\n",
    "    if (os.path.splitext(file))[1] == '.mp4':\n",
    "        cap = cv2.VideoCapture(fr\"{test_path}\\{file}\")\n",
    "        while cap.isOpened():\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "            cv2.imshow('frame', img)\n",
    "            '''value = [[0, 0, 0]] * len(columnNames)\n",
    "            value[0] = labelList[\"หมู\"] \n",
    "            i = 1\n",
    "            for index, frame in enumerate(img):\n",
    "                i = 1 + (len(poseColumnNameList) + len(handColumnNameList) * 2) * index\n",
    "                frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=(frame))\n",
    "                poseResult = poseLandmarker.detect(frame)\n",
    "                handResult = HandLandmarker.detect(frame)\n",
    "                poseCoordinates = poseResult.pose_landmarks\n",
    "                handCoordinates = handResult.hand_landmarks\n",
    "                if poseCoordinates != []:\n",
    "                    cv2.imwrite(img)\n",
    "            '''\n",
    "            cap.release()\n",
    "\n",
    "    #except Exception:\n",
    "        #print('not a video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 36-37: truncated \\UXXXXXXXX escape (2582673475.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(handResult.handedness[0][0].category_name)'''\u001b[0m\n\u001b[1;37m                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 36-37: truncated \\UXXXXXXXX escape\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''img = mp.Image.create_from_file(r\"C:\\Users\\mteer\\OneDrive\\Desktop\\hum.png\")\n",
    "\n",
    "value = [[0, 0, 0]] * len(columnNames)\n",
    "value[0] = labelList[\"หมู\"] \n",
    "poseResult = poseLandmarker.detect(img)\n",
    "handResult = HandLandmarker.detect(img)\n",
    "#if poseResult.pose_landmarks != []:\n",
    "#    print('pass')\n",
    "poseCoordinates = poseResult.pose_landmarks\n",
    "handCoordinates = handResult.hand_landmarks\n",
    "\n",
    "print(handResult.handedness[0][0].category_name)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
